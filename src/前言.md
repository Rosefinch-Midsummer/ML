# 前言

## Machine Learning definition

- Arthur Samuel(1959)Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.
- TomMitchell(1998)Well-posed Learning Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.

机器学习（Machine Learning，ML）是多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等。机器学习专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，使之不断改善自身的性能。

机器学习能让我们从数据集中受到启发，换句话说，我们会利用计算机来彰显数据背后的真实含义，这才是机器学习的真实含义。它既不是只会模仿的机器人，也不是具有人类感情的仿生人。



## 学习资源——在线课程

[Coursera: Machine Learning](https://www.coursera.org/learn/machine-learning)

[机器学习-白板推导系列-开篇](https://www.bilibili.com/video/av31950221/)

[Stanford CS229: Machine Learning](https://cs229.stanford.edu/)

[UCB CS189: Introduction to Machine Learning](https://www.eecs189.org/)

## 学习资源——书籍

《统计学习方法（第2版）》分为监督学习和无监督学习两篇，全面系统地介绍了统计学习的主要方法。包括感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场，以及聚类方法、奇异值分解、主成分分析、潜在语义分析、概率潜在语义分析、马尔可夫链蒙特卡罗法、潜在狄利克雷分配和PageRank算法等。


## 学习路线

以下不是针对回答评论，而是对现在初学者一上来就推荐统计学习方法的回答的评价（来自豆瓣）  
  
初学者不适合看《统计学习方法》，但是从事相关行业的人必定要看，本书精简不啰嗦，面面俱到，从原理上给你整得明明白白的，辅以适当的例子，没有多余的图表，因为人工智能不是什么画图跑demo的专业，你需要有扎实的数学基础。  

建议路线，ng课程入门，知道有哪些算法，大致怎么做，然后去kaggle打个入门赛，别做特征工程，把会的算法全用上。然后放下比赛，开始读这本书，同时看机器学习基石或其他比较数学化的进阶课程，这一步不需要你敲代码，你要会的是滚瓜烂熟的推导，做到这一步，再去kaggle参加奖金赛，阅读kernel，学习state of the art 模型，学习特征工程，再在学习过程中阅读最新的论文或者经典的论文，不断迭代这个过程，别淹死在什么机器学习实战上，有现成的轮子不用，非得费那个劲，除非你科班毕业，代码能力扎实，不然你能不能从头实现一遍决策树对你找不找到工作没有任何一毛钱关系。笔试不会考你如何实现hmm，只会考数据结构与算法，面试只会让你推导。

面过一些学过吴恩达公开课和《统计学习方法》之类教材的同学，一方面在实现上，C++水平不行，然后就想要来做算法，说实话，看过这些东西对于机器学习的面有所了解。了解人家转述的算法，连论文都没读过就想要来做研究也是醉了。说PCA，对于SVD分解的方法讲的头头是道，但是问他怎么用基本的特征分解来做，就说自己不了解。说K均值和EM好像很熟悉，问他K均值跟EM什么关系，不知道。说EM熟悉吧，高斯混合模型不了解……说梯度下降很熟悉吧，问他什么是线性最小二乘法什么是非线性最小二乘法，都支支吾吾不懂。一言以蔽之，知其然而不知其所以然，要做算法研究就踏踏实实以书本为纲，找到感兴趣的点去读相关论文。要做算法实现就老老实实地把C++学好，只会一点点python加上读过两本普及教材说真的没有公司要招你。

个人认为各种ML算法（含监督式/非监督式/强化学习）难度从小到大以及学习的顺序应该是这样的：

入门：线性回归/逻辑回归，kNN，k-means，决策树，神经网络（MLP/CNN/LSTM），反向传播，RL的bandit/MDP/动态规划/TD/MC

初级：贝叶斯线性回归/逻辑回归，NLP的Transformer/BERT/GPT, GMM/HMM，EM/Baum-Welch/Viterbi/卡尔曼滤波，概率图模型和BP，Junction tree，SVM/核方法/RKHS，model-based RL/policy gradient/actor-critic

中级：helmholtz/boltzmann machine/RBM/DBN，高斯过程，t-SNE/manifold learning/非线性降维，各种近似推断（平均场，期望传播，变分贝叶斯，loopy BP，kikuchi，stein），各种采样和MCMC方法（MH, Gibb’s, HMC, SMC, AIS, particle filter)，各种积分方法（MC/高斯数值求积/无迹变换）。以及一些核方法延伸的主题如MMD，HSIC之类的

高级：统计学习理论（ERM/VC维/PAC/rademacher等），贝叶斯非参（DP/CRP/IBP等）

另外附一份（个人认为）按以上各“等级”的数学要求：

入门：多元微积分，线性代数，基本概率论（随机变量，基本分布，期望和方差）

初级：凸优化（尤其constrained opt/KKT condition），图论，高维概率论（尤其是正态分布，包括正态分布的线性变换，条件概率分布），贝叶斯推断，随机过程

中级：更多的概率论（指数族分布和GLM，skewness, kurtosis，测度论）和凸优化，统计力学（spin glass model），凸分析，泛函分析








