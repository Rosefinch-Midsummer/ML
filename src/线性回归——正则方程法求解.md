# 线性回归——正则方程法求解

<!-- toc -->

假设数据集为：
$$
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
$$
其中$x_i\in R^p, y_i\in R, i=1,2,...,N$

考虑截距的话，则$x_i\in R^{p+1}, y_i\in R, i=1,2,...,N$，此时X、W和Y的表达式如下：

$$\left.X=\left(\begin{matrix}{x_{1}}&{x_{2}}&{\cdots}&{x_{N}}\\\end{matrix}\right.\right)^{T}=\left(\begin{matrix}{x_{1}^{T}}\\{x_{2}^{T}}\\{\vdots}\\{x_{N}^{T}}\\\end{matrix}\right)=\left(\begin{matrix}{1}&{x_{11}}&{x_{12}}&{\cdots}&{x_{1p}}\\{1}&{x_{21}}&{x_{22}}&{\cdots}&{x_{2p}}\\{\vdots}&{\vdots}&{\vdots}&{\vdots}&{\vdots}\\{1}&{x_{N1}}&{x_{N2}}&{\cdots}&{x_{Np}}\\\end{matrix}\right)_{N*（p+1）}$$




$$W = \left(\begin{matrix}{w_{0}}\\{w_{1}}\\{w_{2}}\\{\vdots}\\{w_{p}}\\\end{matrix}\right)_{(p+1)\times1}$$

$$Y = \left(\begin{matrix}{y_{1}}\\{y_{2}}\\{\vdots}\\{y_{N}}\\\end{matrix}\right)_{N\times1}$$


方便起见，后面我们记：
$$
X=(x_1,x_2,\cdots,x_N)^T,Y=(y_1,y_2,\cdots,y_N)^T
$$
对**每一个样本数据**，作出如下的线性回归假设：
$$
f(w)=w^Tx
$$

## Least squares revisited

Armed with the tools of matrix derivatives, let us now proceed to find in closed-form the value of _θ_ that minimizes _J(θ)_. We begin by re-writing _J_ in matrix-vectorial notation.

Given a training set, define the design matrix X to be the n-by-d matrix (actually n-by-d + 1, if we include the intercept term) that contains the training examples’ input values in its rows:

$$\left.X=\left[\begin{array}{c}-(x^{(1)})^T-\\-(x^{(2)})^T-\\\vdots\\-(x^{(n)})^T-\end{array}\right.\right].$$

Also, let $\vec{y}$ be the $n$-dimensional vector containing all the target values from the training set:

$$\left.\vec{y}=\left[\begin{array}{c}y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}\end{array}\right.\right].$$

Now, since $h_{\theta}( x^{( i) }) = ( x^{( i) }) ^{T}\theta,$ we can easily verify that

![](https://cdn.jsdelivr.net/gh/Rosefinch-Midsummer/MyImagesHost03/img/202312311134157.png)

Thus, using the fact that for a vector $z,$ we have that $z^Tz= \sum _iz_i^2{: }$

$$
\begin{aligned}\frac12(X\theta-\vec{y})^T(X\theta-\vec{y})&&=&\frac12\sum_{i=1}^n(h_\theta(x^{(i)})-y^{(i)})^2\\&&=&J(\theta)\end{aligned}
$$


Finally, to minimize $J$ let's find its derivatives with respect to $\theta$. Hence,

$$
\begin{aligned}
\nabla_{\theta}J(\theta)& =\quad\nabla_\theta\frac12(X\theta-\vec{y})^T(X\theta-\vec{y}) \\
&=\quad\frac{1}{2}\nabla_{\theta}\:\big((X\theta)^TX\theta-(X\theta)^T\vec{y}-\vec{y}^T(X\theta)+\vec{y}^T\vec{y}\big) \\
&=\quad\frac{1}{2}\nabla_{\theta}\:\left(\theta^T(X^TX)\theta-\vec{y}^T(X\theta)-\vec{y}^T(X\theta)\right) \\
&=\begin{array}{rcl}\frac12\nabla_\theta&\left(\theta^T(X^TX)\theta-2(X^T\vec{y})^T\theta\right)\end{array} \\
&=\quad\frac12\left(2X^TX\theta-2X^T\vec{y}\right) \\
&\begin{array}{rcl}=&X^TX\theta-X^T\vec{y}\end{array}
\end{aligned}
$$
 In the third step, we used the fact that $a^Tb=b^Ta$, and in the fifth step used the facts $\nabla_xb^Tx= b$ and $\nabla_xx^TAx= 2Ax$ for symmetric matrix $A$ (fon more details, see Section 4.3 of “Linear Algebra Review and Reference”). To minimize $J,$ we set its derivatives to zero, and obtain the normal equations:

$$
X^TX\theta=X^T\vec{y}
$$

Thus, the value of $\theta$ that minimizes $J(\theta)$ is given in closed form by the equation

$$
\theta=(X^TX)^{-1}X^T\vec{y}
$$

## 最小二乘法

对这个问题，采用二范数定义的平方误差来定义损失函数：
$$
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
$$
展开得到：
$$
\begin{align}
L(w)&=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\\
&=(w^TX^T-Y^T)\cdot (Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\\
&=w^TX^TXw-2w^TX^TY+Y^TY
\end{align}
$$
最小化这个值的 $\hat{w}$ ：
$$
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)&\longrightarrow\frac{\partial}{\partial w}L(w)=0\nonumber\\
&\longrightarrow2X^TX\hat{w}-2X^TY=0\nonumber\\
&\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{align}
$$
这个式子中 $(X^TX)^{-1}X^T$ 又被称为伪逆。对于行满秩或者列满秩的 $X$，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法，对 $X$ 求奇异值分解，得到
$$
X=U\Sigma V^T
$$
于是：
$$
X^+=V\Sigma^{-1}U^T
$$
在几何上，最小二乘法相当于模型（这里就是直线）和试验值的距离的平方求和，假设我们的试验样本张成一个 $p$ 维空间（满秩的情况）：$X=Span(x_1,\cdots,x_N)$，而模型可以写成 $f(w)=X\beta$，也就是 $x_1,\cdots,x_N$ 的某种组合，而最小二乘法就是说希望 $Y$ 和这个模型距离越小越好，于是它们的差应该与这个张成的空间垂直：
$$
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
$$

## 噪声为高斯分布的 MLE

对于一维的情况，记 $y=w^Tx+\epsilon,\epsilon\sim\mathcal{N}(0,\sigma^2)$，那么 $y\sim\mathcal{N}(w^Tx,\sigma^2)$。代入极大似然估计中：
$$
\begin{align}
L(w)=\log p(Y|X,w)&=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\nonumber\\
&=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\\
\mathop{argmax}\limits_wL(w)&=\mathop{argmin}\limits_w\sum\limits_{i=1^N}(y_i-w^Tx_i)^2
\end{align}
$$
这个表达式和最小二乘估计得到的结果一样。

## 权重先验也为高斯分布的 MAP

取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是： 
$$
\begin{align}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&=\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\\
&=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\\
&=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw]
\end{align}
$$
这里省略了 $X$，$p(Y)$和 $w$ 没有关系，同时也利用了上面高斯分布的 MLE的结果。

我们将会看到，超参数 $\sigma_0$的存在和下面会介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。









